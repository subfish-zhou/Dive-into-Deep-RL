<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>动手学强化学习</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="title_page.html">动手学深度强化学习</a></li><li class="chapter-item expanded "><a href="Introduction.html"><strong aria-hidden="true">1.</strong> 介绍</a></li><li class="chapter-item expanded "><a href="REINFORCE.html"><strong aria-hidden="true">2.</strong> REINFORCE算法</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">动手学强化学习</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        <a href="https://github.com/subfish-zhou/Dive-into-Deep-RL" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#动手学深度强化学习" id="动手学深度强化学习">动手学深度强化学习</a></h1>
<p><em>作者：<a href="https://github.com/subfish-zhou">子鱼</a></em></p>
<p>你现在正在阅读的是本教程的0.1alpha版本，可能过于简明，未来的更新将会逐步添加内容并提升友好度，如果可能的话还会加入最新的研究进展。以CC BY-NC-SA 4.0协议共享。</p>
<p>本教程基于Pytorch实现，所有代码在Python 3.8.5 + Pytorch 1.8.1+cu111环境下运行通过。我们假设读者熟悉深度学习，Pytorch，并对于基本的强化学习思想有“名词党”式的了解。没有看过Barto和Sutton的经典《强化学习》砖头书的同学也不要害怕，本教程不会太注重数学细节，而注重算法的想法和程序实现。</p>
<p>欢迎对本译文提出宝贵意见，可邮件至<a href="mailto:subfishzhou@gmail.com">subfishzhou@gmail.com</a></p>
<h1><a class="header" href="#介绍" id="介绍">介绍</a></h1>
<h2><a class="header" href="#强化学习简介" id="强化学习简介">强化学习简介</a></h2>
<p><strong>总问题</strong>：在环境中为实现优化目标做出序列决策。</p>
<p><img src="1.1.png" alt="强化学习整体框架" />
图1：强化学习MDP问题整体框架</p>
<p><strong>要素</strong>：状态\( s_t\in \mathcal{S} \)，动作\( a_t\in\mathcal{A} \)，奖励\( r_t\in\mathcal{R} \)。</p>
<p><strong>条件</strong>：马尔可夫性：后一个状态仅依赖于前一个状态，即\( s_{t+1}\sim P(s_{t+1}|s_t,a_t) \)。满足此性质的决策问题称为马尔可夫决策过程（Markov decision process，MDP），绝大多数的决策问题都满足此条件，本教程也只考虑MDP问题的求解。</p>
<p><strong>要学习的函数</strong></p>
<p>一个智能体为了实现最大化的总回报，要做出一系列的决策。这些决策可以用一些函数来表示和生成：</p>
<ul>
<li>策略函数\( \pi:\mathcal{S}\rightarrow\mathcal{A} \)，表示智能体在某状态下要执行何种动作的函数。</li>
<li>价值函数\( V^{\pi}(s) \)或\( Q^{\pi}(s,a) \)，作为期望收益\( \mathbb{E}_\tau[R(\tau) ] \)的估计。</li>
<li>环境模型\( P(s' | s,a) \)，表示在某种状态下做出某种动作时向下一个状态转移的概率。</li>
</ul>
<p>策略函数是最直接的生成策略的依据。例如AlphaGo的走子网络：根据当前棋盘局面（状态）判断下一手的位置。价值函数表示的是每一种可能的状态或行动能带来多大收益，用于间接生成策略，生成的方式可以是贪心的——每一次行动都按照当前最大的收益来实施，也可以是\( \epsilon \)-贪心的——由于价值函数的估计并不一定反映真实情况，我们只以\( \epsilon \)的概率实施贪心策略，同时以\( 1-\epsilon \)的概率探索新策略来丰富价值函数的估计。环境模型表示智能体对环境的认识，并由此能够预测自己的行动能够带来什么影响。</p>
<h2><a class="header" href="#深度强化学习算法" id="深度强化学习算法">深度强化学习算法</a></h2>
<p>深度强化学习对经典强化学习的提升在于，使用“最强函数拟合器”深度神经网络来表示要学习的函数。根据不同算法使用深度神经网络来表示的函数不同，Deep-RL算法可以分成如下的类别：</p>
<p>基于策略的方法：</p>
<ul>
<li>REINFORCE</li>
</ul>
<p>基于价值的方法：</p>
<ul>
<li>SARSA</li>
<li>DQN：Deep Q-Network</li>
<li>Double DQN</li>
<li>DQN + Prioritized Experience Replay 先验经验回放</li>
<li>QT-OPT</li>
</ul>
<p>基于模型的方法：</p>
<ul>
<li>iLQR：Iterative Linear Quadratic Regulator</li>
<li>MPC：Model Predictive Control</li>
<li>MCTS：Monte Carlo Tree Search</li>
</ul>
<p>价值和策略结合的方法：</p>
<ul>
<li>Actor-Critic：
<ul>
<li>A2C：Advantage Actor-Critic</li>
<li>GAE：Actor-Critic with Generalized Advantage Estimation</li>
<li>A3C：Asynchronous Advantage Actor-Critic</li>
</ul>
</li>
<li>TRPO：Trust Region Policy Optimization</li>
<li>PPO：Proximal Policy Optimization</li>
<li>SAC：Soft Actor-Critic</li>
</ul>
<p>模型和价值或和策略结合的方法：</p>
<ul>
<li>Dyna-Q / Dyna-AC</li>
<li>AlphaZero</li>
<li>I2A：Imagination Augmented Agents</li>
<li>VPN：Value Prediction Networks</li>
</ul>
<h1><a class="header" href="#reinforce算法" id="reinforce算法">REINFORCE算法</a></h1>
<p>REINFORCE算法是由Ronald J. Williams在1992年的论文《联结主义强化学习的简单统计梯度跟踪算法》（Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning）中提出的基于策略的算法。</p>
<p>REINFORCE算法的想法很自然：在学习过程中，产生高收益的行动应该多做些，导致坏结果的行动应该少做点。我们希望成功的学习结果会让策略函数收敛到能在环境中表现最好的情况。</p>
<p>通过这个想法，我们即将设计的算法会有三个主要组件：</p>
<ol>
<li>一个策略函数，里面有一些参数，描述了某个状态下执行各种动作的概率。这些参数在学习过程中将要被更新；</li>
<li>一个要最大化的目标，能给策略提供即时评价；</li>
<li>一个参数更新方法。</li>
</ol>
<p>这其中策略函数\( \pi_\theta \)用深度神经网络来表示，优化目标\( J(\pi_\theta) \)就是所有完整动作轨迹的期望奖励：</p>
<p>\[ J(\pi_{\theta})=\mathbb{E}<em>{\tau \sim \pi</em>{\theta}}[R(\tau)]=\mathbb{E}<em>{\tau \sim \pi</em>{\theta}}\left[\sum_{t=0}^{T} \gamma^{t} r_{t}\right] \]</p>
<p>该式中\(\tau\sim\pi_\theta\)指是在策略\( \pi_\theta \)的全部样本动作轨迹\( \tau \)上计算期望\(\mathbb{E}\)；\( R_t(\tau) \)是一个轨迹的回报：</p>
<p>\[ R_{t}(\tau)=\sum_{t^{\prime}=t}^{T} \gamma^{t^{\prime}-t} r_{t}^{\prime} \]</p>
<p>需要详细说明的是参数更新方法。策略函数中动作概率的更新是由策略梯度实现的，因此REINFORCE又被称为策略梯度算法。我们以下面的方式更新参数\( \theta \)：</p>
<p>\[ \theta \leftarrow \theta+\alpha \nabla_{\theta} J(\pi_{\theta})\]</p>
<p>其中\( \alpha\in[0,1] \)是学习率，项\( \nabla_{\theta} J(\pi_{\theta}) \)被称为策略梯度，其表达式为：</p>
<p>\[ \nabla_{\theta} J(\pi_{\theta})=\mathbb{E}<em>{\tau \sim \pi</em>{\theta}}\left[\sum_{t=0}^{T} R_{t}(\tau) \nabla_{\theta} \log \pi_{\theta}(a_{t} \mid s_{t})\right] \]</p>
<p>其中\( \pi_{\theta}(a_{t} | s_{t}) \)是智能体在时间步\(t\)处在状态\(s_t\)时，采取行动\(a_t\)的概率。这个表达式的推导可见本节的末尾。直观地看，当某个时间步的回报\( R_t(\tau)&gt;0 \)时，行动概率\( \pi_{\theta}(a_{t} | s_{t}) \)就会上升，反之则下降。</p>
<p>策略梯度的计算需要取期望，为了估计这个期望，我们需要采样。理论上样本越多，估计就越精确，但是对于实际的算法实现来说，一次采样和多次采样是等价的，这是因为实际的训练是分幕式进行的，在一幕中采样多次和多幕采样一次并没有计算量差别。因此就有如下的估计：</p>
<p>\[ \nabla_{\theta} J(\pi_{\theta}) \approx \sum_{t=0}^{T} R_{t}(\tau) \nabla_{\theta} \log \pi_{\theta}(a_{t} \mid s_{t}) \]</p>
<p>现在我们已经做好了充足的准备来写出完整的算法了：</p>
<blockquote>
<p>初始化：学习率\( \alpha \)，权重\( \theta \)，策略网络\( \pi_\theta \)
<strong>for</strong> episode = 0, . . . , maxEpisode <strong>do</strong>:
 生成一条行动轨迹 \(\tau \) = \( s_0, a_0, r_0,\dots, s_T , a_T , r_T\)
 \( \nabla_{\theta} J(\pi_{\theta}) \) = 0
 <strong>for</strong>  t = 0, ..., T <strong>do</strong>
  \( R_{t}(\tau)=\sum_{t^{\prime}=t}^{T} \gamma^{t^{\prime}-t} r_{t}^{\prime} \)
  \(\nabla_{\theta} J(\pi_{\theta}) = J(\pi_{\theta}) + R_{t}(\tau) \nabla_{\theta} \log \pi_{\theta}(a_{t} | s_{t})\)
 <strong>end for</strong>
 \( \theta = \theta+\alpha \nabla_{\theta} J(\pi_{\theta})\)
<strong>end for</strong></p>
</blockquote>
<p>接下来我们用Pytorch来实现这个算法。</p>
<pre><code class="language-Python">from torch.distributions import Categorical
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
gamma = 0.99

class Pi(nn.Module):
    def __init__(self, in_dim, out_dim):
        super(Pi, self).__init__()
        layers = [
            nn.Linear(in_dim, 64),
            nn.ReLU(),
            nn.Linear(64, out_dim),
        ]
        self.model = nn.Sequential(*layers)
        self.onpolicy_reset()
        self.train() # set training mode

    def onpolicy_reset(self):
        self.log_probs = []
        self.rewards = []

    def forward(self, x):
        pdparam = self.model(x) 
        return pdparam

    def act(self, state):
        x = torch.from_numpy(state.astype(np.float32)) # to tensor
        pdparam = self.forward(x) # forward pass
        pd = Categorical(logits=pdparam) # probability distribution
        action = pd.sample() # pi(a|s) in action via pd
        log_prob = pd.log_prob(action) # log_prob of pi(a|s)
        self.log_probs.append(log_prob) # store for training
        return action.item()

    def train(pi, optimizer):
        # Inner gradient-ascent loop of REINFORCE algorithm
        T = len(pi.rewards)
        rets = np.empty(T, dtype=np.float32) # the returns
        future_ret = 0.0
        # compute the returns efficiently
        for t in reversed(range(T)):
            future_ret = pi.rewards[t] + gamma * future_ret
            rets[t] = future_ret
        rets = torch.tensor(rets)
        log_probs = torch.stack(pi.log_probs)
        loss = - log_probs * rets # gradient term; Negative for maximizing
        loss = torch.sum(loss)
        optimizer.zero_grad()
        loss.backward() # backpropagate, compute gradients
        optimizer.step() # gradient-ascent, update the weights
        return loss

    def main():
        env = gym.make('CartPole-v0')
        in_dim = env.observation_space.shape[0] # 4
        out_dim = env.action_space.n # 2
        pi = Pi(in_dim, out_dim) # policy pi_theta for REINFORCE
        optimizer = optim.Adam(pi.parameters(), lr=0.01)
        for epi in range(300):
            state = env.reset()
            for t in range(200): # cartpole max timestep is 200
                action = pi.act(state)
                state, reward, done, _ = env.step(action)
                pi.rewards.append(reward)
                env.render()
                if done:
                    break
            loss = train(pi, optimizer) # train per episode
            total_reward = sum(pi.rewards)
            solved = total_reward &gt; 195.0
            pi.onpolicy_reset() # onpolicy: clear memory after training
            print(f'Episode {epi}, loss: {loss}, \
            total_reward: {total_reward}, solved: {solved}')

if __name__ == '__main__':
    main()
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
